import os
import numpy as np
import cv2
from PIL import Image
import onnxruntime as ort
import base64
from io import BytesIO
import logging  # æ·»åŠ loggingæ¨¡å—
import re  # æ·»åŠ reæ¨¡å—ä»¥æ”¯æŒæ­£åˆ™è¡¨è¾¾å¼æ“ä½œ

# æƒ…ç»ªæ ‡ç­¾ - ä¸åŸæ¨¡å‹ä¿æŒä¸€è‡´
emotion_labels = {
    0: "sad",        # æ‚²ä¼¤
    1: "disgust",    # åŒæ¶
    2: "angry",      # ç”Ÿæ°”
    3: "neutral",    # ä¸­æ€§
    4: "fear",       # ææƒ§
    5: "surprise",   # æƒŠè®¶
    6: "happy"       # é«˜å…´
}

# ä¸­æ–‡æƒ…ç»ªæ ‡ç­¾
emotion_labels_cn = {
    0: "æ‚²ä¼¤",        # æ‚²ä¼¤
    1: "åŒæ¶",        # åŒæ¶
    2: "ç”Ÿæ°”",        # ç”Ÿæ°”
    3: "ä¸­æ€§",        # ä¸­æ€§
    4: "ææƒ§",       # ææƒ§
    5: "æƒŠè®¶",   # æƒŠè®¶
    6: "é«˜å…´"       # é«˜å…´
}

class EmotionDetectorONNX:
    """ä½¿ç”¨ONNXæ¨¡å‹çš„æƒ…ç»ªæ£€æµ‹å™¨"""
    
    def __init__(self, onnx_model_path, processor_config_path=None):
        """
        åˆå§‹åŒ–æƒ…ç»ªæ£€æµ‹å™¨
        Args:
            onnx_model_path: ONNXæ¨¡å‹æ–‡ä»¶è·¯å¾„
            processor_config_path: å¤„ç†å™¨é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        print("ğŸ” æ­£åœ¨åŠ è½½ONNXæ¨¡å‹...")
        
        # åˆ›å»ºONNX Runtimeä¼šè¯
        self.session = ort.InferenceSession(onnx_model_path)
        
        # è·å–è¾“å…¥è¾“å‡ºåç§°
        self.input_name = self.session.get_inputs()[0].name
        self.output_name = self.session.get_outputs()[0].name
        
        # ä»é…ç½®æ–‡ä»¶åŠ è½½é¢„å¤„ç†å‚æ•°
        if processor_config_path and os.path.exists(processor_config_path):
            import json
            with open(processor_config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            self.image_mean = np.array(config.get("image_mean", [0.5, 0.5, 0.5]))
            self.image_std = np.array(config.get("image_std", [0.5, 0.5, 0.5]))
            self.do_normalize = config.get("do_normalize", True)
            self.do_rescale = config.get("do_rescale", True)
            self.rescale_factor = config.get("rescale_factor", 1/255.0)
        else:
            # ä½¿ç”¨ä»æ¨¡å‹é…ç½®æ–‡ä»¶ä¸­æå–çš„é»˜è®¤å‚æ•°
            self.image_mean = np.array([0.5, 0.5, 0.5])
            self.image_std = np.array([0.5, 0.5, 0.5])
            self.do_normalize = True
            self.do_rescale = True
            self.rescale_factor = 0.00392156862745098  # 1/255
            
        # åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨
        self.face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        
        print("âœ… ONNXæ¨¡å‹åŠ è½½æˆåŠŸ!")

    def detect_face_from_base64(self, image_base64):
        """
        ä»base64ç¼–ç çš„å›¾åƒæ•°æ®æ£€æµ‹æ˜¯å¦å­˜åœ¨äººè„¸
        Args:
            image_base64: base64ç¼–ç çš„å›¾åƒæ•°æ®
        Returns:
            bool: æ˜¯å¦æ£€æµ‹åˆ°äººè„¸
        """
        try:
            # å°†base64å›¾ç‰‡æ•°æ®è½¬æ¢ä¸ºå›¾åƒ
            image_data = base64.b64decode(image_base64)
            image = Image.open(BytesIO(image_data))
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è½¬æ¢ä¸ºBGRæ ¼å¼
            image_np = np.array(image)
            if len(image_np.shape) == 3:
                # RGB to BGR
                image_np = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)
            
            # è½¬æ¢ä¸ºç°åº¦å›¾è¿›è¡Œäººè„¸æ£€æµ‹
            gray = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)
            
            # æ£€æµ‹äººè„¸
            faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
            
            # è¿”å›æ˜¯å¦æ£€æµ‹åˆ°äººè„¸
            return len(faces) > 0
            
        except Exception as e:
            print(f"âŒ äººè„¸æ£€æµ‹å¤±è´¥: {e}")
            return False

    def softmax(self, x):
        """ä½¿ç”¨numpyå®ç°softmaxå‡½æ•°"""
        e_x = np.exp(x - np.max(x))  # ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œå‡å»æœ€å¤§å€¼
        return e_x / e_x.sum(axis=0)

    def preprocess_image(self, image):
        """
        é¢„å¤„ç†å›¾åƒä»¥åŒ¹é…ViTæ¨¡å‹çš„è¾“å…¥è¦æ±‚
        æ ¹æ®æ¨¡å‹é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°è¿›è¡Œç²¾ç¡®é¢„å¤„ç†
        """
        # è°ƒæ•´å›¾åƒå¤§å°ä¸º224x224
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        elif isinstance(image, bytes):
            # å¦‚æœè¾“å…¥æ˜¯å­—èŠ‚æµï¼Œè½¬æ¢ä¸ºPILå›¾åƒ
            image = Image.open(BytesIO(image)).convert('RGB')
        elif hasattr(image, 'read') and callable(getattr(image, 'read')):
            # å¦‚æœè¾“å…¥æ˜¯æ–‡ä»¶å¯¹è±¡ï¼Œè½¬æ¢ä¸ºPILå›¾åƒ
            image = Image.open(image).convert('RGB')
        
        # è°ƒæ•´å¤§å°
        image = image.resize((224, 224))
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        image_array = np.array(image).astype(np.float32)
        
        # è½¬æ¢ä¸ºCHWæ ¼å¼ï¼ˆé€šé“ï¼Œé«˜åº¦ï¼Œå®½åº¦ï¼‰åœ¨å½’ä¸€åŒ–ä¹‹å‰
        image_array = np.transpose(image_array, (2, 0, 1))
        
        # åº”ç”¨é‡ç¼©æ”¾å› å­ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.do_rescale:
            image_array = image_array * self.rescale_factor
            
        # å½’ä¸€åŒ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.do_normalize:
            # è°ƒæ•´å‡å€¼å’Œæ ‡å‡†å·®çš„å½¢çŠ¶ä»¥é€‚åº”å›¾åƒæ•°ç»„ (3, 224, 224)
            mean = self.image_mean.reshape(-1, 1, 1)
            std = self.image_std.reshape(-1, 1, 1)
            image_array = (image_array - mean) / std
        
        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦å¹¶ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32
        image_array = np.expand_dims(image_array, axis=0).astype(np.float32)
        
        return image_array

    def predict_emotion_from_base64(self, image_base64):
        """
        ä»base64ç¼–ç çš„å›¾åƒæ•°æ®é¢„æµ‹æƒ…ç»ª
        Args:
            image_base64: base64ç¼–ç çš„å›¾åƒæ•°æ®
        Returns:
            emotion: è‹±æ–‡æƒ…ç»ªæ ‡ç­¾
            emotion_cn: ä¸­æ–‡æƒ…ç»ªæ ‡ç­¾
            confidence: ç½®ä¿¡åº¦
            emotion_details: å„æƒ…ç»ªç±»åˆ«æ¦‚ç‡
        """
        try:
            # å°†base64å›¾ç‰‡æ•°æ®è½¬æ¢ä¸ºå›¾åƒ
            image_data = base64.b64decode(image_base64)
            image = Image.open(BytesIO(image_data))
            
            # é¢„å¤„ç†å›¾åƒ
            processed_image = self.preprocess_image(image)
            
            # è¿è¡Œæ¨ç†
            result = self.session.run([self.output_name], {self.input_name: processed_image})
            
            # å¤„ç†è¾“å‡º
            logits = result[0][0]
            # ä½¿ç”¨è‡ªå®šä¹‰softmaxå‡½æ•°ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
            probabilities = self.softmax(logits)
            
            # è·å–é¢„æµ‹ç»“æœ
            predicted_class = int(np.argmax(probabilities))
            confidence = float(probabilities[predicted_class])
            emotion = emotion_labels[predicted_class]
            emotion_cn = emotion_labels_cn[predicted_class]
            
            # åˆ›å»ºè¯¦ç»†ç»“æœå­—å…¸
            emotion_details = {}
            for i, prob in enumerate(probabilities):
                emotion_details[emotion_labels_cn[i]] = float(prob)
            
            return emotion, emotion_cn, confidence, emotion_details
            
        except Exception as e:
            print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
            return None, None, 0, None

    def get_emotion_analysis_text(self, image_base64):
        """
        è·å–æƒ…ç»ªåˆ†æç»“æœçš„æ–‡æœ¬æè¿°
        Args:
            image_base64: base64ç¼–ç çš„å›¾åƒæ•°æ®
        Returns:
            emotion_text: æƒ…ç»ªåˆ†æç»“æœæ–‡æœ¬
        """
        emotion, emotion_cn, confidence, emotion_details = self.predict_emotion_from_base64(image_base64)
        
        if emotion is not None:
            # æ„å»ºè¯¦ç»†çš„æƒ…ç»ªåˆ†æç»“æœ
            # result_text = f"æƒ…ç»ªåˆ†æç»“æœï¼š"
            # result_text += f"ä¸»è¦æƒ…ç»ªï¼š{emotion_cn}ï¼ˆç½®ä¿¡åº¦ï¼š{confidence:.3f}ï¼‰\n"
            result_text = f"{emotion_cn}ï¼ˆç½®ä¿¡åº¦ï¼š{confidence:.3f}ï¼‰"
            # result_text += "å„æƒ…ç»ªç±»åˆ«æ¦‚ç‡ï¼š\n"
            
            # æŒ‰æ¦‚ç‡ä»é«˜åˆ°ä½æ’åºæ˜¾ç¤º
            # sorted_emotions = sorted(emotion_details.items(), key=lambda x: x[1], reverse=True)
            # for emotion_name, prob in sorted_emotions:
                # result_text += f"  {emotion_name}ï¼š{prob:.4f} ({prob*100:.1f}%)\n"
                # result_text += f"  {emotion_name}ï¼š{prob:.4f} ({prob*100:.1f}%)\n"

                
            return result_text
        else:
            return "æƒ…ç»ªæ£€æµ‹å¤±è´¥"

    def predict_emotion(self, image):
        """é¢„æµ‹å›¾åƒçš„æƒ…ç»ª"""
        try:
            # é¢„å¤„ç†å›¾åƒ
            processed_image = self.preprocess_image(image)
            
            # è¿è¡Œæ¨ç†
            result = self.session.run([self.output_name], {self.input_name: processed_image})
            
            # å¤„ç†è¾“å‡º
            logits = result[0][0]
            # ä½¿ç”¨è‡ªå®šä¹‰softmaxå‡½æ•°ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
            probabilities = self.softmax(logits)
            
            # è·å–é¢„æµ‹ç»“æœ
            predicted_class = int(np.argmax(probabilities))
            confidence = float(probabilities[predicted_class])
            emotion = emotion_labels[predicted_class]
            
            return emotion, confidence, probabilities
            
        except Exception as e:
            print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
            return None, 0, None

    def estimate_head_pose_influence(self, image):
        """
        ä¼°è®¡å¤´éƒ¨å§¿æ€å¯¹æƒ…ç»ªè¯†åˆ«çš„å½±å“
        è¿”å›ä¸€ä¸ªè°ƒæ•´ç³»æ•°ï¼Œç”¨äºä¿®æ­£æƒ…ç»ªé¢„æµ‹ç»“æœ
        """
        # è¿™é‡Œå¯ä»¥é›†æˆä¸€ä¸ªå¤´éƒ¨å§¿æ€ä¼°è®¡æ¨¡å‹
        # ç›®å‰è¿”å›é»˜è®¤å€¼1.0è¡¨ç¤ºæ— å½±å“
        return 1.0

    def predict_emotion_with_pose_correction(self, image):
        """
        ç»“åˆå¤´éƒ¨å§¿æ€æ ¡æ­£çš„æƒ…ç»ªé¢„æµ‹
        """
        emotion, confidence, probabilities = self.predict_emotion(image)
        
        if emotion is not None:
            # ä¼°è®¡å§¿æ€å½±å“
            pose_influence = self.estimate_head_pose_influence(image)
            
            # å¯¹ç‰¹å®šæƒ…ç»ªï¼ˆå¦‚æ‚²ä¼¤ï¼‰è¿›è¡Œå§¿æ€æ ¡æ­£
            if emotion == "sad" and confidence > 0.5:
                # å¦‚æœè¯†åˆ«ä¸ºæ‚²ä¼¤ä¸”ç½®ä¿¡åº¦è¾ƒé«˜ï¼Œä½†å¯èƒ½æ˜¯ç”±äºä½å¤´å§¿æ€é€ æˆçš„
                adjusted_confidence = confidence * pose_influence
                return emotion, adjusted_confidence, probabilities
            
        return emotion, confidence, probabilities

    def predict_emotion_from_path(self, image_path):
        """ä»å›¾åƒè·¯å¾„é¢„æµ‹æƒ…ç»ª"""
        try:
            print(f"ğŸ“· åŠ è½½å›¾åƒ: {image_path}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(image_path):
                print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                return None, 0, None
                
            # é¢„å¤„ç†å’Œé¢„æµ‹
            emotion, confidence, probabilities = self.predict_emotion_with_pose_correction(image_path)
            
            if emotion is not None:
                # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
                print(f"\nğŸ­ æƒ…ç»ªåˆ†æç»“æœ:")
                print("=" * 50)
                for i, prob in enumerate(probabilities):
                    marker = " ğŸ¯" if i == np.argmax(probabilities) else "   "
                    print(f"{marker} {emotion_labels[i]:8}: {prob:.4f} ({prob*100:.1f}%)")
                print("=" * 50)
                print(f"ğŸ“Š ä¸»è¦æƒ…ç»ª: {emotion}")
                print(f"âœ… ç½®ä¿¡åº¦: {confidence:.3f} ({confidence*100:.1f}%)")
                
            return emotion, confidence, probabilities
            
        except Exception as e:
            print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
            return None, 0, None

    def real_time_detection(self):
        """å®æ—¶æ‘„åƒå¤´æƒ…ç»ªæ£€æµ‹"""
        print("\nğŸ¥ å¯åŠ¨å®æ—¶æƒ…ç»ªæ£€æµ‹...")
        print("æŒ‰ 'q' é€€å‡ºæ‘„åƒå¤´")
        
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
            return
        
        # åŠ è½½äººè„¸æ£€æµ‹å™¨
        face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        
        # ç”¨äºä¿å­˜äººè„¸çš„è®¡æ•°å™¨
        face_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # è½¬æ¢ä¸ºç°åº¦å›¾è¿›è¡Œäººè„¸æ£€æµ‹
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = face_cascade.detectMultiScale(gray, 1.1, 4)
            
            for (x, y, w, h) in faces:
                # æå–äººè„¸åŒºåŸŸ
                face_roi = frame[y:y+h, x:x+w]
                
                try:
                    # è½¬æ¢æ ¼å¼å¹¶é¢„æµ‹
                    face_pil = Image.fromarray(cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB))
                    emotion, confidence, _ = self.predict_emotion(face_pil)
                    
                    if emotion is not None:
                        # æ ¹æ®æƒ…ç»ªé€‰æ‹©é¢œè‰²
                        color_map = {
                            "happy": (0, 255, 0),      # ç»¿è‰²
                            "surprise": (255, 255, 0), # é»„è‰²
                            "neutral": (255, 165, 0),  # æ©™è‰²
                            "sad": (0, 0, 255),        # çº¢è‰²
                            "angry": (0, 0, 255),      # çº¢è‰²
                            "fear": (128, 0, 128),     # ç´«è‰²
                            "disgust": (165, 42, 42)   # æ£•è‰²
                        }
                        color = color_map.get(emotion, (0, 255, 0))
                        
                        # ç»˜åˆ¶ç»“æœ
                        cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)
                        label = f"{emotion} ({confidence:.2f})"
                        cv2.putText(frame, label, (x, y-10), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
                        
                        # ä¿å­˜è¯†åˆ«åˆ°çš„äººè„¸åˆ° emotion_face æ–‡ä»¶å¤¹
                        if not os.path.exists("emotion_face"):
                            os.makedirs("emotion_face")
                            
                        face_filename = f"emotion_face/face_{face_count}_{emotion}.jpg"
                        cv2.imwrite(face_filename, face_roi)
                        face_count += 1
                    
                except Exception as e:
                    print(f"å¤„ç†äººè„¸æ—¶å‡ºé”™: {e}")
            
            # æ˜¾ç¤ºç”»é¢
            cv2.imshow('å®æ—¶æƒ…ç»ªè¯†åˆ« - æŒ‰ q é€€å‡º', frame)
            
            # æŒ‰qé€€å‡º
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()

    def predict_folder_images(self, folder_path, output_file="emotion_results_onnx.txt"):
        """è¯†åˆ«æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰å›¾ç‰‡çš„æƒ…ç»ªå¹¶ä¿å­˜ç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶"""
        if not os.path.exists(folder_path):
            print(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
            return
        
        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        supported_formats = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.tif')
        
        # è·å–æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        image_files = [f for f in os.listdir(folder_path) 
                       if f.lower().endswith(supported_formats) and 
                       os.path.isfile(os.path.join(folder_path, f))]
        
        if not image_files:
            print(f"âŒ åœ¨ {folder_path} ä¸­æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡")
        
        # æ„å»ºè¾“å‡ºæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        output_path = os.path.join(folder_path, output_file)
        
        # æ‰“å¼€ç»“æœæ–‡ä»¶å‡†å¤‡å†™å…¥
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(f"æƒ…ç»ªè¯†åˆ«ç»“æœ (ONNXç‰ˆæœ¬)\n")
            f.write(f"æ–‡ä»¶å¤¹: {folder_path}\n")
            f.write(f"æ€»å…± {len(image_files)} å¼ å›¾ç‰‡\n")
            f.write("=" * 50 + "\n\n")
            
            # å¤„ç†æ¯å¼ å›¾ç‰‡
            for i, image_file in enumerate(image_files, 1):
                image_path = os.path.join(folder_path, image_file)
                print(f"\nå¤„ç†è¿›åº¦: {i}/{len(image_files)} - {image_file}")
                
                try:
                    # é¢„æµ‹æƒ…ç»ª
                    emotion, confidence, probabilities = self.predict_emotion_from_path(image_path)
                    
                    if emotion is not None:
                        # è¾“å‡ºåˆ°æ§åˆ¶å°
                        print(f"  ğŸ“Š æƒ…ç»ª: {emotion} (ç½®ä¿¡åº¦: {confidence:.3f})")
                        
                        # å†™å…¥åˆ°æ–‡ä»¶
                        f.write(f"å›¾ç‰‡ {i}: {image_file}\n")
                        f.write(f"æƒ…ç»ª: {emotion}\n")
                        f.write(f"ç½®ä¿¡åº¦: {confidence:.3f} ({confidence*100:.1f}%)\n")
                        
                        # å†™å…¥è¯¦ç»†æ¦‚ç‡
                        f.write("å„ç±»åˆ«æ¦‚ç‡:\n")
                        for j, prob in enumerate(probabilities):
                            marker = " >>> " if j == np.argmax(probabilities) else "     "
                            f.write(f"{marker}{emotion_labels[j]:8}: {prob:.4f} ({prob*100:.1f}%)\n")
                        f.write("\n")
                    else:
                        f.write(f"å›¾ç‰‡ {i}: {image_file}\n")
                        f.write("é”™è¯¯: æ— æ³•å¤„ç†è¯¥å›¾ç‰‡\n\n")
                        
                except Exception as e:
                    error_msg = f"å¤„ç†å›¾ç‰‡ {image_file} æ—¶å‡ºé”™: {e}"
                    print(f"âŒ {error_msg}")
                    f.write(f"å›¾ç‰‡ {i}: {image_file}\n")
                    f.write(f"é”™è¯¯: {error_msg}\n\n")
        
        print(f"\nâœ… æ‰€æœ‰å›¾ç‰‡å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° {output_path}")

def initialize_emotion_detector():
    """
    åˆå§‹åŒ–æƒ…ç»ªæ£€æµ‹å™¨
    """
    try:
        # ä½¿ç”¨ç›¸å¯¹äºå½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„æ¥å®šä½æ¨¡å‹æ–‡ä»¶
        import os
        current_file_dir = os.path.dirname(os.path.abspath(__file__))
        model_path = os.path.join(current_file_dir, "emotion_model.onnx")
        
        if not os.path.exists(model_path):
            print(f"âŒ æƒ…ç»ªæ£€æµ‹æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return None
        
        # åˆå§‹åŒ–æƒ…ç»ªæ£€æµ‹å™¨
        processor_config_path = os.path.join(current_file_dir, "train_emotion_model/disgust_fine_tuned_model_1.1/preprocessor_config.json")
        detector = EmotionDetectorONNX(model_path, processor_config_path)
        return detector
    except Exception as e:
        print(f"åˆå§‹åŒ–æƒ…ç»ªæ£€æµ‹å™¨å¤±è´¥: {e}")
        return None

# å…¨å±€æƒ…ç»ªæ£€æµ‹å™¨å®ä¾‹ï¼Œå®ç°ä¸€æ¬¡åŠ è½½å¤šæ¬¡ä½¿ç”¨
EMOTION_DETECTOR = initialize_emotion_detector()

def clean_multimodal_text(text):
    """
    æ¸…ç†å¤šæ¨¡æ€APIè¿”å›çš„æ–‡æœ¬ï¼Œç§»é™¤ç³»ç»Ÿæ ‡ç­¾å’Œå†—ä½™æè¿°
    """
    if not text:
        return text
    
    # ç§»é™¤æ‰€æœ‰å½¢å¦‚<|.*?|>çš„æ ‡ç­¾åºåˆ—
    cleaned_text = re.sub(r'<\|[\w]+\|>', '', text)
    
    # ç§»é™¤"æƒ…ç»ªè¯†åˆ«ç»“æœ:"ç­‰å†—ä½™æè¿°
    cleaned_text = re.sub(r'æƒ…ç»ªè¯†åˆ«ç»“æœ:\s*', '', cleaned_text)
    
    # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
    cleaned_text = cleaned_text.strip()
    
    return cleaned_text


def detect_emotion(image_base64, include_prefix=True):
    """
    ä½¿ç”¨æœ¬åœ°æƒ…ç»ªæ£€æµ‹æ¨¡å‹æ£€æµ‹å›¾ç‰‡ä¸­çš„æƒ…ç»ª
    Args:
        image_base64: base64ç¼–ç çš„å›¾ç‰‡æ•°æ®
        api_key: APIå¯†é’¥
        model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        include_prefix: æ˜¯å¦åŒ…å«"æƒ…ç»ªåˆ†æï¼š"å‰ç¼€
    Returns:
        æƒ…ç»ªåˆ†æç»“æœæ–‡æœ¬
    """
    if EMOTION_DETECTOR:
        try:
            # ä½¿ç”¨æœ¬åœ°æƒ…ç»ªæ£€æµ‹æ¨¡å‹
            emotion_result = get_emotion_analysis_text(image_base64)
            
            # è¿‡æ»¤æ‰ä¸éœ€è¦çš„å†…å®¹
            if emotion_result:
                # æ¸…ç†æƒ…ç»ªè¯†åˆ«ç»“æœ
                filtered_result = clean_multimodal_text(emotion_result)
                # å¦‚æœç»“æœä¸ºç©ºï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
                result = filtered_result if filtered_result else ""
                
                # æ ¹æ®include_prefixå‚æ•°å†³å®šæ˜¯å¦æ·»åŠ å‰ç¼€
                if include_prefix and result:
                    result = "æƒ…ç»ªåˆ†æï¼š" + result
                return result
            return ""
        except Exception as e:
            logging.error(f"æœ¬åœ°æƒ…ç»ªæ£€æµ‹å‡ºé”™: {str(e)}")
            return ""
    else:
        # å¦‚æœæœ¬åœ°æƒ…ç»ªæ£€æµ‹ä¸å¯ç”¨ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
        logging.error("æœ¬åœ°æƒ…ç»ªæ£€æµ‹æ¨¡å—æœªåˆå§‹åŒ–")
        return ""


def get_emotion_analysis_text(image_base64):
    """
    ä¾›å¤–éƒ¨è°ƒç”¨çš„æƒ…ç»ªåˆ†æå‡½æ•°
    Args:
        image_base64: base64ç¼–ç çš„å›¾åƒæ•°æ®
    Returns:
        emotion_text: æƒ…ç»ªåˆ†æç»“æœæ–‡æœ¬
    """
    if EMOTION_DETECTOR is None:
        return "æƒ…ç»ªæ£€æµ‹å™¨æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨"
    
    try:
        # å…ˆè¿›è¡Œå›¾åƒåˆ†ææ˜¯å¦å­˜åœ¨äººè„¸
        if not EMOTION_DETECTOR.detect_face_from_base64(image_base64):
            return ""
        return EMOTION_DETECTOR.get_emotion_analysis_text(image_base64)
    except Exception as e:
        print(f"æƒ…ç»ªæ£€æµ‹å‡ºé”™: {str(e)}")
        return ""

def main():
    print("ğŸ¯ ONNXäººè„¸æƒ…ç»ªè¯†åˆ«ç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆå§‹åŒ–æƒ…ç»ªæ£€æµ‹å™¨ï¼Œä¼ å…¥é¢„å¤„ç†å™¨é…ç½®æ–‡ä»¶è·¯å¾„
    detector = EMOTION_DETECTOR
    
    if detector is None:
        print("âŒ æ— æ³•åˆå§‹åŒ–æƒ…ç»ªæ£€æµ‹å™¨")
        return
    
    while True:
        print("\nè¯·é€‰æ‹©æ¨¡å¼:")
        print("1. ğŸ“· æµ‹è¯•å•å¼ å›¾ç‰‡")
        print("2. ğŸ¥ å®æ—¶æ‘„åƒå¤´æ£€æµ‹")
        print("3. ğŸ“ è¯†åˆ«æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰å›¾ç‰‡æƒ…ç»ª")
        print("4. ğŸšª é€€å‡º")
        
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()
        
        if choice == '1':
            image_path = input("è¯·è¾“å…¥å›¾ç‰‡è·¯å¾„: ").strip()
            detector.predict_emotion_from_path(image_path)
        
        elif choice == '2':
            detector.real_time_detection()
        
        elif choice == '3':
            folder_path = input("è¯·è¾“å…¥å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„: ").strip()
            output_file = input("è¯·è¾“å…¥ç»“æœä¿å­˜æ–‡ä»¶å (é»˜è®¤ä¸º emotion_results_onnx.txt): ").strip()
            if not output_file:
                output_file = "emotion_results_onnx.txt"
            detector.predict_folder_images(folder_path, output_file)
        
        elif choice == '4':
            print("ğŸ‘‹ å†è§!")
            break
        
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

if __name__ == "__main__":
    main()